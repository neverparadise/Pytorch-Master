{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a935b7fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\train-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80075430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "torch.Size([60000, 28, 28])\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset)) # 첫 번째 데이터의 개수를 파악한다.\n",
    "print(train_dataset.data.shape)# 두 번째 데이터의 shape를 파악한다.\n",
    "print(train_dataset.classes) # 세 번째 데이터의 클래스들을 파악한다.\n",
    "# 네 번째 모델의 입력과 출력 사이즈를 정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba6b50",
   "metadata": {},
   "source": [
    "## 퀴즈 (Easy)  \n",
    "Multi Layered Perceptrion 을 통해 MNIST 데이터셋을 분류하려면  \n",
    "1) 모델의 첫 번째 레이어의 shape 784  \n",
    "2) 모델의 출력크기는 어떻게 되어야할까요?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46bdfa5",
   "metadata": {},
   "source": [
    "## 퀴즈 (Normal)  \n",
    "간단한 MLP 모델을 구현해봅시다. \n",
    "1) 레이어 수는 총 4개로 (784, 512, 256, 128) 개의 뉴런이 존재합니다. \n",
    "  \n",
    "  \n",
    "2) 활성화함수는 relu를 사용합니다.  \n",
    "\n",
    "3) forward 함수에 x를 처음 입력받을 때 기존에 배운 flatten() 또는 reshape() 또는 view()를 활용해서 일차원 벡터로 변환하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae49f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화함수 사용하는 방법\n",
    "# x = F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the data (batch_size, 1, 28, 28)-> (batch_size, 784)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec2207",
   "metadata": {},
   "source": [
    "이제 train, test 함수를 작성해보겠습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3463939f",
   "metadata": {},
   "source": [
    "## train, test 함수 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1c193525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "model = Model().to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "def train(epoch, model, loss_func, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "            ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "\n",
    "def test(model, loss_func, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_count = 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        test_loss += loss_func(y_pred, y).item()\n",
    "        pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "        # torch.eq : Computes element-wise equality. return counts value\n",
    "        correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count/len(test_loader.dataset):.3}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae4ebe",
   "metadata": {},
   "source": [
    "## train 및 test 실행  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f9834776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      " Test set: Average loss: 0.0721, Accuracy: 0.0792\n"
     ]
    }
   ],
   "source": [
    "test(model, ce_loss, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "35e0616c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.305856\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 1.419130\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 1.501759\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (16% | Loss: 1.493733\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 1.636190\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (27% | Loss: 1.415927\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (32% | Loss: 1.680676\n",
      "Train Epoch: 1 | Batch Status: 22400/60000             (37% | Loss: 1.743567\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (43% | Loss: 1.593692\n",
      "Train Epoch: 1 | Batch Status: 28800/60000             (48% | Loss: 1.253008\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (53% | Loss: 1.345199\n",
      "Train Epoch: 1 | Batch Status: 35200/60000             (59% | Loss: 1.477875\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (64% | Loss: 1.600612\n",
      "Train Epoch: 1 | Batch Status: 41600/60000             (69% | Loss: 1.778572\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (75% | Loss: 1.374381\n",
      "Train Epoch: 1 | Batch Status: 48000/60000             (80% | Loss: 1.490510\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (85% | Loss: 1.335951\n",
      "Train Epoch: 1 | Batch Status: 54400/60000             (91% | Loss: 1.743988\n",
      "Train Epoch: 1 | Batch Status: 57600/60000             (96% | Loss: 1.606024\n",
      "=======================\n",
      " Test set: Average loss: 0.0459, Accuracy: 0.379\n",
      "Train Epoch: 2 | Batch Status: 0/60000             (0% | Loss: 1.659296\n",
      "Train Epoch: 2 | Batch Status: 3200/60000             (5% | Loss: 1.691025\n",
      "Train Epoch: 2 | Batch Status: 6400/60000             (11% | Loss: 1.584778\n",
      "Train Epoch: 2 | Batch Status: 9600/60000             (16% | Loss: 1.271998\n",
      "Train Epoch: 2 | Batch Status: 12800/60000             (21% | Loss: 1.469921\n",
      "Train Epoch: 2 | Batch Status: 16000/60000             (27% | Loss: 1.655186\n",
      "Train Epoch: 2 | Batch Status: 19200/60000             (32% | Loss: 1.529161\n",
      "Train Epoch: 2 | Batch Status: 22400/60000             (37% | Loss: 1.381274\n",
      "Train Epoch: 2 | Batch Status: 25600/60000             (43% | Loss: 1.153277\n",
      "Train Epoch: 2 | Batch Status: 28800/60000             (48% | Loss: 1.391069\n",
      "Train Epoch: 2 | Batch Status: 32000/60000             (53% | Loss: 1.445346\n",
      "Train Epoch: 2 | Batch Status: 35200/60000             (59% | Loss: 1.532617\n",
      "Train Epoch: 2 | Batch Status: 38400/60000             (64% | Loss: 1.512715\n",
      "Train Epoch: 2 | Batch Status: 41600/60000             (69% | Loss: 1.360081\n",
      "Train Epoch: 2 | Batch Status: 44800/60000             (75% | Loss: 1.325587\n",
      "Train Epoch: 2 | Batch Status: 48000/60000             (80% | Loss: 1.349782\n",
      "Train Epoch: 2 | Batch Status: 51200/60000             (85% | Loss: 1.584788\n",
      "Train Epoch: 2 | Batch Status: 54400/60000             (91% | Loss: 1.551263\n",
      "Train Epoch: 2 | Batch Status: 57600/60000             (96% | Loss: 1.397302\n",
      "=======================\n",
      " Test set: Average loss: 0.0459, Accuracy: 0.378\n",
      "Train Epoch: 3 | Batch Status: 0/60000             (0% | Loss: 1.655307\n",
      "Train Epoch: 3 | Batch Status: 3200/60000             (5% | Loss: 1.517216\n",
      "Train Epoch: 3 | Batch Status: 6400/60000             (11% | Loss: 1.441634\n",
      "Train Epoch: 3 | Batch Status: 9600/60000             (16% | Loss: 1.224054\n",
      "Train Epoch: 3 | Batch Status: 12800/60000             (21% | Loss: 1.513595\n",
      "Train Epoch: 3 | Batch Status: 16000/60000             (27% | Loss: 1.626706\n",
      "Train Epoch: 3 | Batch Status: 19200/60000             (32% | Loss: 1.439129\n",
      "Train Epoch: 3 | Batch Status: 22400/60000             (37% | Loss: 1.301782\n",
      "Train Epoch: 3 | Batch Status: 25600/60000             (43% | Loss: 1.328188\n",
      "Train Epoch: 3 | Batch Status: 28800/60000             (48% | Loss: 1.642345\n",
      "Train Epoch: 3 | Batch Status: 32000/60000             (53% | Loss: 1.008493\n",
      "Train Epoch: 3 | Batch Status: 35200/60000             (59% | Loss: 1.726946\n",
      "Train Epoch: 3 | Batch Status: 38400/60000             (64% | Loss: 1.203526\n",
      "Train Epoch: 3 | Batch Status: 41600/60000             (69% | Loss: 1.369422\n",
      "Train Epoch: 3 | Batch Status: 44800/60000             (75% | Loss: 1.483490\n",
      "Train Epoch: 3 | Batch Status: 48000/60000             (80% | Loss: 1.378837\n",
      "Train Epoch: 3 | Batch Status: 51200/60000             (85% | Loss: 1.375249\n",
      "Train Epoch: 3 | Batch Status: 54400/60000             (91% | Loss: 1.367639\n",
      "Train Epoch: 3 | Batch Status: 57600/60000             (96% | Loss: 1.704963\n",
      "=======================\n",
      " Test set: Average loss: 0.0455, Accuracy: 0.381\n",
      "Train Epoch: 4 | Batch Status: 0/60000             (0% | Loss: 1.367998\n",
      "Train Epoch: 4 | Batch Status: 3200/60000             (5% | Loss: 1.448159\n",
      "Train Epoch: 4 | Batch Status: 6400/60000             (11% | Loss: 1.093089\n",
      "Train Epoch: 4 | Batch Status: 9600/60000             (16% | Loss: 1.331091\n",
      "Train Epoch: 4 | Batch Status: 12800/60000             (21% | Loss: 1.154818\n",
      "Train Epoch: 4 | Batch Status: 16000/60000             (27% | Loss: 1.367323\n",
      "Train Epoch: 4 | Batch Status: 19200/60000             (32% | Loss: 1.298434\n",
      "Train Epoch: 4 | Batch Status: 22400/60000             (37% | Loss: 1.223441\n",
      "Train Epoch: 4 | Batch Status: 25600/60000             (43% | Loss: 1.511181\n",
      "Train Epoch: 4 | Batch Status: 28800/60000             (48% | Loss: 1.054105\n",
      "Train Epoch: 4 | Batch Status: 32000/60000             (53% | Loss: 1.584871\n",
      "Train Epoch: 4 | Batch Status: 35200/60000             (59% | Loss: 1.368269\n",
      "Train Epoch: 4 | Batch Status: 38400/60000             (64% | Loss: 1.299704\n",
      "Train Epoch: 4 | Batch Status: 41600/60000             (69% | Loss: 1.455929\n",
      "Train Epoch: 4 | Batch Status: 44800/60000             (75% | Loss: 1.520444\n",
      "Train Epoch: 4 | Batch Status: 48000/60000             (80% | Loss: 1.627647\n",
      "Train Epoch: 4 | Batch Status: 51200/60000             (85% | Loss: 1.511622\n",
      "Train Epoch: 4 | Batch Status: 54400/60000             (91% | Loss: 1.372751\n",
      "Train Epoch: 4 | Batch Status: 57600/60000             (96% | Loss: 1.442885\n",
      "=======================\n",
      " Test set: Average loss: 0.0454, Accuracy: 0.381\n",
      "Train Epoch: 5 | Batch Status: 0/60000             (0% | Loss: 1.463825\n",
      "Train Epoch: 5 | Batch Status: 3200/60000             (5% | Loss: 1.727635\n",
      "Train Epoch: 5 | Batch Status: 6400/60000             (11% | Loss: 1.223528\n",
      "Train Epoch: 5 | Batch Status: 9600/60000             (16% | Loss: 1.511669\n",
      "Train Epoch: 5 | Batch Status: 12800/60000             (21% | Loss: 1.444384\n",
      "Train Epoch: 5 | Batch Status: 16000/60000             (27% | Loss: 1.583049\n",
      "Train Epoch: 5 | Batch Status: 19200/60000             (32% | Loss: 1.409743\n",
      "Train Epoch: 5 | Batch Status: 22400/60000             (37% | Loss: 1.488319\n",
      "Train Epoch: 5 | Batch Status: 25600/60000             (43% | Loss: 1.594226\n",
      "Train Epoch: 5 | Batch Status: 28800/60000             (48% | Loss: 1.367510\n",
      "Train Epoch: 5 | Batch Status: 32000/60000             (53% | Loss: 1.080003\n",
      "Train Epoch: 5 | Batch Status: 35200/60000             (59% | Loss: 1.007607\n",
      "Train Epoch: 5 | Batch Status: 38400/60000             (64% | Loss: 1.459805\n",
      "Train Epoch: 5 | Batch Status: 41600/60000             (69% | Loss: 1.655086\n",
      "Train Epoch: 5 | Batch Status: 44800/60000             (75% | Loss: 1.670424\n",
      "Train Epoch: 5 | Batch Status: 48000/60000             (80% | Loss: 1.295934\n",
      "Train Epoch: 5 | Batch Status: 51200/60000             (85% | Loss: 1.726425\n",
      "Train Epoch: 5 | Batch Status: 54400/60000             (91% | Loss: 1.439219\n",
      "Train Epoch: 5 | Batch Status: 57600/60000             (96% | Loss: 1.441641\n",
      "=======================\n",
      " Test set: Average loss: 0.0456, Accuracy: 0.379\n",
      "Train Epoch: 6 | Batch Status: 0/60000             (0% | Loss: 1.620816\n",
      "Train Epoch: 6 | Batch Status: 3200/60000             (5% | Loss: 1.511831\n",
      "Train Epoch: 6 | Batch Status: 6400/60000             (11% | Loss: 1.583029\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22792/3898387838.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22792/2873586918.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, model, loss_func, train_loader, optimizer)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m255\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     \u001b[1;31m# put it from HWC to CHW format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(0,3):\n",
    "    train(epoch, model, ce_loss, train_loader, optimizer)\n",
    "    test(model, ce_loss, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67959d1",
   "metadata": {},
   "source": [
    "# MNIST Classification with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "679d7cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "6\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(C=1, W=28, H=28, K=3, S=2) \n",
    "cnn = cnn.to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "75245488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98250\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in cnn.parameters())\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0e3934f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.302427\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 1.108058\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 1.009592\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (16% | Loss: 0.905163\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 1.115762\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (27% | Loss: 1.071527\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (32% | Loss: 1.084060\n",
      "Train Epoch: 1 | Batch Status: 22400/60000             (37% | Loss: 1.142990\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (43% | Loss: 0.954704\n",
      "Train Epoch: 1 | Batch Status: 28800/60000             (48% | Loss: 1.082639\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (53% | Loss: 1.232279\n",
      "Train Epoch: 1 | Batch Status: 35200/60000             (59% | Loss: 1.085640\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (64% | Loss: 1.037536\n",
      "Train Epoch: 1 | Batch Status: 41600/60000             (69% | Loss: 1.032761\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (75% | Loss: 1.016086\n",
      "Train Epoch: 1 | Batch Status: 48000/60000             (80% | Loss: 0.717318\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (85% | Loss: 0.860540\n",
      "Train Epoch: 1 | Batch Status: 54400/60000             (91% | Loss: 1.017245\n",
      "Train Epoch: 1 | Batch Status: 57600/60000             (96% | Loss: 1.226743\n",
      "=======================\n",
      " Test set: Average loss: 0.0304, Accuracy: 0.594\n",
      "Train Epoch: 2 | Batch Status: 0/60000             (0% | Loss: 0.659492\n",
      "Train Epoch: 2 | Batch Status: 3200/60000             (5% | Loss: 0.791925\n",
      "Train Epoch: 2 | Batch Status: 6400/60000             (11% | Loss: 0.871884\n",
      "Train Epoch: 2 | Batch Status: 9600/60000             (16% | Loss: 0.944634\n",
      "Train Epoch: 2 | Batch Status: 12800/60000             (21% | Loss: 0.865619\n",
      "Train Epoch: 2 | Batch Status: 16000/60000             (27% | Loss: 0.721624\n",
      "Train Epoch: 2 | Batch Status: 19200/60000             (32% | Loss: 0.975217\n",
      "Train Epoch: 2 | Batch Status: 22400/60000             (37% | Loss: 1.040903\n",
      "Train Epoch: 2 | Batch Status: 25600/60000             (43% | Loss: 0.722069\n",
      "Train Epoch: 2 | Batch Status: 28800/60000             (48% | Loss: 0.865219\n",
      "Train Epoch: 2 | Batch Status: 32000/60000             (53% | Loss: 1.166875\n",
      "Train Epoch: 2 | Batch Status: 35200/60000             (59% | Loss: 1.091557\n",
      "Train Epoch: 2 | Batch Status: 38400/60000             (64% | Loss: 1.353006\n",
      "Train Epoch: 2 | Batch Status: 41600/60000             (69% | Loss: 1.081689\n",
      "Train Epoch: 2 | Batch Status: 44800/60000             (75% | Loss: 0.935868\n",
      "Train Epoch: 2 | Batch Status: 48000/60000             (80% | Loss: 1.151770\n",
      "Train Epoch: 2 | Batch Status: 51200/60000             (85% | Loss: 1.161413\n",
      "Train Epoch: 2 | Batch Status: 54400/60000             (91% | Loss: 1.080440\n",
      "Train Epoch: 2 | Batch Status: 57600/60000             (96% | Loss: 0.650080\n",
      "=======================\n",
      " Test set: Average loss: 0.0304, Accuracy: 0.594\n",
      "Train Epoch: 3 | Batch Status: 0/60000             (0% | Loss: 1.311416\n",
      "Train Epoch: 3 | Batch Status: 3200/60000             (5% | Loss: 0.980259\n",
      "Train Epoch: 3 | Batch Status: 6400/60000             (11% | Loss: 1.079947\n",
      "Train Epoch: 3 | Batch Status: 9600/60000             (16% | Loss: 0.772120\n",
      "Train Epoch: 3 | Batch Status: 12800/60000             (21% | Loss: 0.724035\n",
      "Train Epoch: 3 | Batch Status: 16000/60000             (27% | Loss: 0.724188\n",
      "Train Epoch: 3 | Batch Status: 19200/60000             (32% | Loss: 0.695030\n",
      "Train Epoch: 3 | Batch Status: 22400/60000             (37% | Loss: 0.863892\n",
      "Train Epoch: 3 | Batch Status: 25600/60000             (43% | Loss: 1.159414\n",
      "Train Epoch: 3 | Batch Status: 28800/60000             (48% | Loss: 1.020553\n",
      "Train Epoch: 3 | Batch Status: 32000/60000             (53% | Loss: 1.007886\n",
      "Train Epoch: 3 | Batch Status: 35200/60000             (59% | Loss: 1.008010\n",
      "Train Epoch: 3 | Batch Status: 38400/60000             (64% | Loss: 1.354904\n",
      "Train Epoch: 3 | Batch Status: 41600/60000             (69% | Loss: 1.225384\n",
      "Train Epoch: 3 | Batch Status: 44800/60000             (75% | Loss: 1.007815\n",
      "Train Epoch: 3 | Batch Status: 48000/60000             (80% | Loss: 0.864604\n",
      "Train Epoch: 3 | Batch Status: 51200/60000             (85% | Loss: 1.017658\n",
      "Train Epoch: 3 | Batch Status: 54400/60000             (91% | Loss: 0.780009\n",
      "Train Epoch: 3 | Batch Status: 57600/60000             (96% | Loss: 1.091240\n",
      "=======================\n",
      " Test set: Average loss: 0.0302, Accuracy: 0.591\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,3):\n",
    "    train(epoch, cnn, ce_loss, train_loader, optimizer)\n",
    "    test(model, ce_loss, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7c8303e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98250\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c095428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers, block, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # feature map size = 32x32x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 16x16x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 8x8x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        # self.avg_pool = nn.AvgPool2d(8, stride=1)\n",
    "        self.fc_out = nn.Linear(49 * 64, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "\n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "\n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "\n",
    "        #x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "\n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "\n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet_model():\n",
    "    block = ResidualBlock\n",
    "    model = ResNet(1, 3, block)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c4c03895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300154\n"
     ]
    }
   ],
   "source": [
    "resnet = resnet_model()\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "total_params = sum(p.numel() for p in resnet.parameters())\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c78c02ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.986992\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 0.224228\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 0.102577\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (16% | Loss: 0.285066\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 0.185515\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (27% | Loss: 0.221114\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22792/4185915264.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22792/2873586918.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, model, loss_func, train_loader, optimizer)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
      "\u001b[1;32mc:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(0,3):\n",
    "    train(epoch, resnet, ce_loss, train_loader, optimizer)\n",
    "    test(model, ce_loss, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c31d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871838fa",
   "metadata": {},
   "source": [
    "## Assignment  \n",
    "\n",
    "1) cifar10 데이터셋, 데이터로더를 할당하는 코드를 실행해서 데이터셋을 다운받으세요.  \n",
    "2) 데이터셋의 채널과 모양을 확인해보세요. 그리고 채널과 모양은 CNN을 구현할 때 반영하세요.  \n",
    "3) conv layer가 5개인 CNN 모델을 구현하세요 모델의 총 파라미터 수는 약 30만개여야 합니다. 배치정규화를 사용하세요.    \n",
    "4) 구현한 CNN 모델을 학습시키세요. learning_rate=0.001, optimizer=Adam, Epochs=100  \n",
    "5) 구현되어 있는 Resnet 모델을 불러와서 학습시켜보세요. 옵티마이저와 하이퍼파라미터는 기존과 동일합니다.  \n",
    "6) 두 모델 중 어느 것이 더 좋은가요?  \n",
    "7) 필요하다면 구글링을 적극적으로 하세요!!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d6e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
