{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470e1ea9",
   "metadata": {},
   "source": [
    "미니배치, 이터레이션, 에포크 참조  \n",
    "https://wikidocs.net/55580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8e6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634666f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee0ce60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.294118   0.487437   0.180328  ... -0.53117   -0.0333333  0.       ]\n",
      " [-0.882353  -0.145729   0.0819672 ... -0.766866  -0.666667   1.       ]\n",
      " [-0.0588235  0.839196   0.0491803 ... -0.492741  -0.633333   0.       ]\n",
      " ...\n",
      " [-0.411765   0.21608    0.180328  ... -0.857387  -0.7        1.       ]\n",
      " [-0.882353   0.266332  -0.0163934 ... -0.768574  -0.133333   0.       ]\n",
      " [-0.882353  -0.0653266  0.147541  ... -0.797609  -0.933333   1.       ]]\n",
      "(759, 9)\n"
     ]
    }
   ],
   "source": [
    "print(xy)\n",
    "print(xy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b1ed5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, xy_dataset):\n",
    "        # 커스텀 데이셋 클래스의 생성자를 정의\n",
    "        # 데이터를 불러와서 torch.tensor로 할당 및 전처리한다.\n",
    "        self.x_data = torch.from_numpy(xy_dataset[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy_dataset[:, [-1]])\n",
    "        print(f'X shape :{self.x_data.shape} | Y shape :{self.y_data.shape} ')\n",
    "    \n",
    "    # 매직 메소드 : 함수 이름 앞, 뒤로 underbar 2개를 붙인 메소드\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터셋에서 해당 인덱스에 해당하는 샘플 (x, y)를 가져오는 메소드\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ba71b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :torch.Size([759, 8]) | Y shape :torch.Size([759, 1]) \n"
     ]
    }
   ],
   "source": [
    "dataset = DiabetesDataset(xy)\n",
    "# DataLoader 모듈 : 데이터셋과 배치사이즈를 입력받아서 인스턴스를 생성. 학습할 때 미니배치를 구성해준다.\n",
    "data_loader = DataLoader(dataset, batch_size=100, num_workers=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e93d55",
   "metadata": {},
   "source": [
    "## 퀴즈 (Easy)  \n",
    "1) 당뇨병 데이터셋에 대해서 로지스틱 회귀 모델을 구현한다면 첫번째 레이어의 노드 수는 몇개가 되어야 할까요??   \n",
    "2) data_loader에서 샘플들을 가져온다면 샘플들의 전체 모양은 어떻게 될까요?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9bc9ec",
   "metadata": {},
   "source": [
    "## 퀴즈 (Normal)  \n",
    "이전의 노트북 파일을 참고해서 LogisticRegressionModel 클래스를 구현하세요.  \n",
    "1) 생성자 :  \n",
    "모델의 은닉층 수는 3층이고 은닉층마다 (8, 6, 4) 개의 뉴런을 가집니다.  \n",
    "마지막 레이어에는 시그모이드 모듈을 이용해 예측값을 계산하도록 만드세요.  \n",
    "2) forward :   \n",
    "최종적으로 확률 값을 예측하도록 레이어를 쌓아서 y를 반환하세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec0ca16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(8, 6)\n",
    "        self.layer2 = nn.Linear(6, 4)\n",
    "        self.layer3 = nn.Linear(4, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.layer1(x))\n",
    "        x = self.sigmoid(self.layer2(x))\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        y_pred = self.sigmoid(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd26f4",
   "metadata": {},
   "source": [
    "## 퀴즈 (Normal)  \n",
    "1) 위 모델을 학습시키기 위해서는 어떤 손실함수를 선택해야할까요??  \n",
    "2) 위의 로지스틱회귀 클래스 모델을 조금 더 메모리 효율적으로 구현하려면 어떻게 해야할까요?  \n",
    "3) 위의 로지스틱 회귀 모델을 공책에 그려봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdec99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel()\n",
    "bce_loss = nn.BCELoss(reduction='mean')\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bced36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1, 2, 3, 4, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f2a846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, loss_func, data_loader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for batch_index, samples in enumerate(data_loader):\n",
    "            x_train, y_train = samples\n",
    "            output = model(x_train)\n",
    "            optimizer.zero_grad()\n",
    "            L = loss_func(output, y_train) \n",
    "            L.backward() \n",
    "            optimizer.step()\n",
    "            print(f'Epoch: {epoch+1:4d}/{epochs} Batch {batch_index+1:4d}/{len(data_loader)} \\\n",
    "                Loss: {L.item():4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1fad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78bb99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1/10 Batch    1/8                 Loss: 0.655255\n",
      "Epoch:    1/10 Batch    2/8                 Loss: 0.659631\n",
      "Epoch:    1/10 Batch    3/8                 Loss: 0.673315\n",
      "Epoch:    1/10 Batch    4/8                 Loss: 0.664268\n",
      "Epoch:    1/10 Batch    5/8                 Loss: 0.618826\n",
      "Epoch:    1/10 Batch    6/8                 Loss: 0.614176\n",
      "Epoch:    1/10 Batch    7/8                 Loss: 0.645937\n",
      "Epoch:    1/10 Batch    8/8                 Loss: 0.668745\n",
      "Epoch:    2/10 Batch    1/8                 Loss: 0.655032\n",
      "Epoch:    2/10 Batch    2/8                 Loss: 0.659487\n",
      "Epoch:    2/10 Batch    3/8                 Loss: 0.673360\n",
      "Epoch:    2/10 Batch    4/8                 Loss: 0.664185\n",
      "Epoch:    2/10 Batch    5/8                 Loss: 0.618192\n",
      "Epoch:    2/10 Batch    6/8                 Loss: 0.613476\n",
      "Epoch:    2/10 Batch    7/8                 Loss: 0.645639\n",
      "Epoch:    2/10 Batch    8/8                 Loss: 0.668725\n",
      "Epoch:    3/10 Batch    1/8                 Loss: 0.654843\n",
      "Epoch:    3/10 Batch    2/8                 Loss: 0.659363\n",
      "Epoch:    3/10 Batch    3/8                 Loss: 0.673408\n",
      "Epoch:    3/10 Batch    4/8                 Loss: 0.664112\n",
      "Epoch:    3/10 Batch    5/8                 Loss: 0.617587\n",
      "Epoch:    3/10 Batch    6/8                 Loss: 0.612803\n",
      "Epoch:    3/10 Batch    7/8                 Loss: 0.645356\n",
      "Epoch:    3/10 Batch    8/8                 Loss: 0.668713\n",
      "Epoch:    4/10 Batch    1/8                 Loss: 0.654664\n",
      "Epoch:    4/10 Batch    2/8                 Loss: 0.659248\n",
      "Epoch:    4/10 Batch    3/8                 Loss: 0.673461\n",
      "Epoch:    4/10 Batch    4/8                 Loss: 0.664047\n",
      "Epoch:    4/10 Batch    5/8                 Loss: 0.617002\n",
      "Epoch:    4/10 Batch    6/8                 Loss: 0.612153\n",
      "Epoch:    4/10 Batch    7/8                 Loss: 0.645086\n",
      "Epoch:    4/10 Batch    8/8                 Loss: 0.668706\n",
      "Epoch:    5/10 Batch    1/8                 Loss: 0.654495\n",
      "Epoch:    5/10 Batch    2/8                 Loss: 0.659141\n",
      "Epoch:    5/10 Batch    3/8                 Loss: 0.673518\n",
      "Epoch:    5/10 Batch    4/8                 Loss: 0.663988\n",
      "Epoch:    5/10 Batch    5/8                 Loss: 0.616439\n",
      "Epoch:    5/10 Batch    6/8                 Loss: 0.611524\n",
      "Epoch:    5/10 Batch    7/8                 Loss: 0.644828\n",
      "Epoch:    5/10 Batch    8/8                 Loss: 0.668705\n",
      "Epoch:    6/10 Batch    1/8                 Loss: 0.654336\n",
      "Epoch:    6/10 Batch    2/8                 Loss: 0.659042\n",
      "Epoch:    6/10 Batch    3/8                 Loss: 0.673579\n",
      "Epoch:    6/10 Batch    4/8                 Loss: 0.663937\n",
      "Epoch:    6/10 Batch    5/8                 Loss: 0.615895\n",
      "Epoch:    6/10 Batch    6/8                 Loss: 0.610916\n",
      "Epoch:    6/10 Batch    7/8                 Loss: 0.644582\n",
      "Epoch:    6/10 Batch    8/8                 Loss: 0.668709\n",
      "Epoch:    7/10 Batch    1/8                 Loss: 0.654185\n",
      "Epoch:    7/10 Batch    2/8                 Loss: 0.658951\n",
      "Epoch:    7/10 Batch    3/8                 Loss: 0.673643\n",
      "Epoch:    7/10 Batch    4/8                 Loss: 0.663890\n",
      "Epoch:    7/10 Batch    5/8                 Loss: 0.615370\n",
      "Epoch:    7/10 Batch    6/8                 Loss: 0.610329\n",
      "Epoch:    7/10 Batch    7/8                 Loss: 0.644346\n",
      "Epoch:    7/10 Batch    8/8                 Loss: 0.668716\n",
      "Epoch:    8/10 Batch    1/8                 Loss: 0.654042\n",
      "Epoch:    8/10 Batch    2/8                 Loss: 0.658866\n",
      "Epoch:    8/10 Batch    3/8                 Loss: 0.673710\n",
      "Epoch:    8/10 Batch    4/8                 Loss: 0.663850\n",
      "Epoch:    8/10 Batch    5/8                 Loss: 0.614863\n",
      "Epoch:    8/10 Batch    6/8                 Loss: 0.609760\n",
      "Epoch:    8/10 Batch    7/8                 Loss: 0.644121\n",
      "Epoch:    8/10 Batch    8/8                 Loss: 0.668728\n",
      "Epoch:    9/10 Batch    1/8                 Loss: 0.653906\n",
      "Epoch:    9/10 Batch    2/8                 Loss: 0.658787\n",
      "Epoch:    9/10 Batch    3/8                 Loss: 0.673780\n",
      "Epoch:    9/10 Batch    4/8                 Loss: 0.663814\n",
      "Epoch:    9/10 Batch    5/8                 Loss: 0.614374\n",
      "Epoch:    9/10 Batch    6/8                 Loss: 0.609210\n",
      "Epoch:    9/10 Batch    7/8                 Loss: 0.643905\n",
      "Epoch:    9/10 Batch    8/8                 Loss: 0.668743\n",
      "Epoch:   10/10 Batch    1/8                 Loss: 0.653778\n",
      "Epoch:   10/10 Batch    2/8                 Loss: 0.658714\n",
      "Epoch:   10/10 Batch    3/8                 Loss: 0.673851\n",
      "Epoch:   10/10 Batch    4/8                 Loss: 0.663783\n",
      "Epoch:   10/10 Batch    5/8                 Loss: 0.613902\n",
      "Epoch:   10/10 Batch    6/8                 Loss: 0.608678\n",
      "Epoch:   10/10 Batch    7/8                 Loss: 0.643699\n",
      "Epoch:   10/10 Batch    8/8                 Loss: 0.668761\n"
     ]
    }
   ],
   "source": [
    "fit(model, optimizer, bce_loss, data_loader, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c458e",
   "metadata": {},
   "source": [
    "## Dataloader를 활용한 학습  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ae091",
   "metadata": {},
   "source": [
    "## 퀴즈 (Easy)  \n",
    "위 학습 반복문에서 enumerate를 사용했습니다. 그 이유가 무엇일까요?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1bad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
