{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fcd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "kernel_size = 3\n",
    "stride = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784d93bf",
   "metadata": {},
   "source": [
    "## 모듈화  \n",
    "코드의 가독성을 좋게 하고 보수 및 관리를 쉽게 합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc57d444",
   "metadata": {},
   "source": [
    "```\n",
    "+-- configs/\n",
    "|   +-- cnn.yaml\n",
    "+-- dataset\n",
    "|   +-- mnist_data/...\n",
    "|   +-- MNIST_LOADER.py\n",
    "+-- models\n",
    "|   +-- cnn.py\n",
    "|   +-- res_net.py\n",
    "+-- runs\n",
    "|   +-- cnn/...\n",
    "|   +-- experiments/...\n",
    "+-- wetights\n",
    "|   +-- cnn.pth\n",
    "+-- argparse_tutorial.py\n",
    "+-- README.md\n",
    "+-- run_cnn.py\n",
    "+-- run_cnn2.py\n",
    "+-- requirements.txt\n",
    "+-- utils.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafed4c5",
   "metadata": {},
   "source": [
    "## Quiz (Easy)  \n",
    "위 구조와 같이 모듈화를 진행하겠습니다.  \n",
    "1) dataset 폴더를 만들고 mnist_data 폴더를 옮겨주세요. 그리고 MNIST_LOADER.py를 dataset에 만듭니다.   \n",
    "2) models 폴더를 만들고 cnn.py, res_net.py를 만들고 내용을 채워주세요   \n",
    "3) utils 파일을 만들고 conv2d_size_out 함수를 작성하세요. CNN 클래스의 생성자에 정의된 함수를 그대로 옮기면 됩니다.  \n",
    "4) runs 폴더를 만들어주세요.   \n",
    "5) weights 폴더를 만들어주세요.   \n",
    "6) requirements.txt 파일을 만들어주세요.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed66f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "valid_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "vaild_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_func, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "            ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "\n",
    "def test(model, loss_func, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_count = 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        test_loss += loss_func(y_pred, y).item()\n",
    "        pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "        # torch.eq : Computes element-wise equality. return counts value\n",
    "        correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count/len(test_loader.dataset):.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, C, W, H, K, S): # 채널, 너비, 높이, 커널 사이즈, 스트라이드\n",
    "        super(CNN, self).__init__()\n",
    "        # nn.Module에는 이미 conv 레이어가 구현되어 있다. \n",
    "        # 배치정규화도 구현되어있고 다 구현되어있습니다. \n",
    "        self.conv1 = nn.Conv2d(C, 32, kernel_size=K, stride=S)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=K, stride=S)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=K, stride=S)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=K, stride=S):\n",
    "            print((size - (kernel_size - 1) - 1) // stride + 1)\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(W, K, S)\n",
    "        convw = conv2d_size_out(convw, K, S)\n",
    "        convw = conv2d_size_out(convw, K, S)\n",
    "        \n",
    "        self.linear_input_size = convw * convw * 128\n",
    "        self.fc = nn.Linear(self.linear_input_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1) # (batch_size, flatten_size)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61461ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(C=C, W=W, H=H, K=3, S=2) \n",
    "cnn = cnn.to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch, cnn, ce_loss, train_loader, optimizer)\n",
    "\n",
    "test(cnn, ce_loss, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
