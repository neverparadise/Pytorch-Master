{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47fcd13",
   "metadata": {},
   "source": [
    "## 모델 저장과 로드  \n",
    "여러분은 클라우드를 사용하면서 세션이 끊어지는 것을 한 번쯤은 경험해보셨을 것입니다.  \n",
    "이때, 만약 학습한 가중치를 저장하지 않는다면 몇 시간을 학습한 것이 날아갈 것입니다.   \n",
    "이번에는 학습 과정에서 모델을 저장하는 방법과, 학습 전에 모델을 불러오는 방법을 배우겠습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c65df92",
   "metadata": {},
   "source": [
    "## Quiz (Easy)  \n",
    "0) run_cnn2 파일을 만들어서 기존의 코드를 리팩터링 해봅시다.  \n",
    "1) 앞에서 배웠던 argparser를 이용해 config_path, save_path, pre_trained 인자를 추가하세요  \n",
    "2) 상위 폴더에 weights 폴더를 만드세요.   \n",
    "3) save_path의 default 값은 './weights'이고 config_path의 default는 './configs' 입니다.  \n",
    "4) pre_trained의 type은 bool이고 defaut 값은 False 입니다.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae8db0",
   "metadata": {},
   "source": [
    "## Quiz (Easy)  \n",
    "모델을 로드하고 저장하는 부분을 구현하기 위해 train, test 코드를 수정해야 합니다.  \n",
    "아래에서 어떤 부분에 추가해야할까요??  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ea94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_func, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "            ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "\n",
    "def test(model, loss_func, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_count = 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        test_loss += loss_func(y_pred, y).item()\n",
    "        pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "        # torch.eq : Computes element-wise equality. return counts value\n",
    "        correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count/len(test_loader.dataset):.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe36ccd",
   "metadata": {},
   "source": [
    "## Save, Load  \n",
    "모델의 저장과 로드는 torch.load_state_dict(), torch.load(), torch.save()를 활용합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e61e1e4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_trained' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19116/2295130686.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mpre_trained\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mmodel_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pre_trained' is not defined"
     ]
    }
   ],
   "source": [
    "if pre_trained:\n",
    "    model_dict = torch.load(save_path+model_name)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "def train(epoch, model, loss_func, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "            ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "            torch.save(model.state_dict(), save_path + model_name)\n",
    "            \n",
    "\n",
    "def test(model, loss_func, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_count = 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        test_loss += loss_func(y_pred, y).item()\n",
    "        pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "        # torch.eq : Computes element-wise equality. return counts value\n",
    "        correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count/len(test_loader.dataset):.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdafbb6f",
   "metadata": {},
   "source": [
    "## Tensorboard  \n",
    "tensorboard는 모델학습 과정의 loss나 기타 지표를 확인해서 학습이 잘되고 있는지, 모델 테스트 성능이  \n",
    "어떻게 나오는지를 시각화해줍니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb122ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10598347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ee5d0",
   "metadata": {},
   "source": [
    "먼저 runs 폴더를 만들고 그 안에 cnn 폴더를 만들어주세요.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b6dc9",
   "metadata": {},
   "source": [
    "## Quiz (Normal)  \n",
    "add_scalar는 train, test함수에서 어느 줄에 삽입해야 할까요?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_trained:\n",
    "    model_dict = torch.load(save_path+model_name)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "def train(epoch, model, loss_func, train_loader, valid_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        train_loss = loss_func(y_pred, y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "            ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {train_loss.item():.6f}')\n",
    "            torch.save(model.state_dict(), save_path + model_name)\n",
    "\n",
    "    for batch_index, (x, y) in enumerate(valid_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        val_loss = loss_func(y_pred, y)\n",
    "        \n",
    "def test(model, loss_func, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_count = 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        test_loss += loss_func(y_pred, y).item()\n",
    "        pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "        # torch.eq : Computes element-wise equality. return counts value\n",
    "        correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count/len(test_loader.dataset):.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58637bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "pre_trained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0737b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './configs/'\n",
    "save_path = './weights/'\n",
    "model_name = 'cnn.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5ac1e24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "6\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "cnn = CNN(C=1, W=28, H=28, K=3, S=2) \n",
    "cnn = cnn.to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "writer = SummaryWriter('runs/cnn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86cf57e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.293186\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 1.113714\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 1.064491\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (16% | Loss: 0.686955\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 0.965053\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (27% | Loss: 0.459951\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (32% | Loss: 0.819730\n",
      "Train Epoch: 1 | Batch Status: 22400/60000             (37% | Loss: 0.690969\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (43% | Loss: 0.686268\n",
      "Train Epoch: 1 | Batch Status: 28800/60000             (48% | Loss: 0.903146\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (53% | Loss: 0.559102\n",
      "Train Epoch: 1 | Batch Status: 35200/60000             (59% | Loss: 0.894578\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (64% | Loss: 0.765228\n",
      "Train Epoch: 1 | Batch Status: 41600/60000             (69% | Loss: 1.308519\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (75% | Loss: 1.100264\n",
      "Train Epoch: 1 | Batch Status: 48000/60000             (80% | Loss: 0.465319\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (85% | Loss: 0.805286\n",
      "Train Epoch: 1 | Batch Status: 54400/60000             (91% | Loss: 0.646653\n",
      "Train Epoch: 1 | Batch Status: 57600/60000             (96% | Loss: 0.581447\n",
      "Train Epoch: 2 | Batch Status: 0/60000             (0% | Loss: 0.566322\n",
      "Train Epoch: 2 | Batch Status: 3200/60000             (5% | Loss: 0.724684\n",
      "Train Epoch: 2 | Batch Status: 6400/60000             (11% | Loss: 0.772006\n",
      "Train Epoch: 2 | Batch Status: 9600/60000             (16% | Loss: 0.596862\n",
      "Train Epoch: 2 | Batch Status: 12800/60000             (21% | Loss: 0.640563\n",
      "Train Epoch: 2 | Batch Status: 16000/60000             (27% | Loss: 0.721540\n",
      "Train Epoch: 2 | Batch Status: 19200/60000             (32% | Loss: 0.953970\n",
      "Train Epoch: 2 | Batch Status: 22400/60000             (37% | Loss: 0.995232\n",
      "Train Epoch: 2 | Batch Status: 25600/60000             (43% | Loss: 0.865449\n",
      "Train Epoch: 2 | Batch Status: 28800/60000             (48% | Loss: 0.761107\n",
      "Train Epoch: 2 | Batch Status: 32000/60000             (53% | Loss: 0.938200\n",
      "Train Epoch: 2 | Batch Status: 35200/60000             (59% | Loss: 0.826681\n",
      "Train Epoch: 2 | Batch Status: 38400/60000             (64% | Loss: 0.670595\n",
      "Train Epoch: 2 | Batch Status: 41600/60000             (69% | Loss: 0.520668\n",
      "Train Epoch: 2 | Batch Status: 44800/60000             (75% | Loss: 0.938990\n",
      "Train Epoch: 2 | Batch Status: 48000/60000             (80% | Loss: 1.179451\n",
      "Train Epoch: 2 | Batch Status: 51200/60000             (85% | Loss: 0.806625\n",
      "Train Epoch: 2 | Batch Status: 54400/60000             (91% | Loss: 0.577565\n",
      "Train Epoch: 2 | Batch Status: 57600/60000             (96% | Loss: 0.745916\n",
      "Train Epoch: 3 | Batch Status: 0/60000             (0% | Loss: 0.970593\n",
      "Train Epoch: 3 | Batch Status: 3200/60000             (5% | Loss: 0.440729\n",
      "Train Epoch: 3 | Batch Status: 6400/60000             (11% | Loss: 0.654588\n",
      "Train Epoch: 3 | Batch Status: 9600/60000             (16% | Loss: 0.940330\n",
      "Train Epoch: 3 | Batch Status: 12800/60000             (21% | Loss: 0.720394\n",
      "Train Epoch: 3 | Batch Status: 16000/60000             (27% | Loss: 0.511379\n",
      "Train Epoch: 3 | Batch Status: 19200/60000             (32% | Loss: 0.370843\n",
      "Train Epoch: 3 | Batch Status: 22400/60000             (37% | Loss: 0.589331\n",
      "Train Epoch: 3 | Batch Status: 25600/60000             (43% | Loss: 0.791945\n",
      "Train Epoch: 3 | Batch Status: 28800/60000             (48% | Loss: 0.870673\n",
      "Train Epoch: 3 | Batch Status: 32000/60000             (53% | Loss: 0.801235\n",
      "Train Epoch: 3 | Batch Status: 35200/60000             (59% | Loss: 1.007694\n",
      "Train Epoch: 3 | Batch Status: 38400/60000             (64% | Loss: 0.648749\n",
      "Train Epoch: 3 | Batch Status: 41600/60000             (69% | Loss: 0.938329\n",
      "Train Epoch: 3 | Batch Status: 44800/60000             (75% | Loss: 0.882594\n",
      "Train Epoch: 3 | Batch Status: 48000/60000             (80% | Loss: 0.821543\n",
      "Train Epoch: 3 | Batch Status: 51200/60000             (85% | Loss: 0.737517\n",
      "Train Epoch: 3 | Batch Status: 54400/60000             (91% | Loss: 1.030945\n",
      "Train Epoch: 3 | Batch Status: 57600/60000             (96% | Loss: 1.131153\n",
      "Train Epoch: 4 | Batch Status: 0/60000             (0% | Loss: 0.588403\n",
      "Train Epoch: 4 | Batch Status: 3200/60000             (5% | Loss: 0.864343\n",
      "Train Epoch: 4 | Batch Status: 6400/60000             (11% | Loss: 0.794986\n",
      "Train Epoch: 4 | Batch Status: 9600/60000             (16% | Loss: 0.576946\n",
      "Train Epoch: 4 | Batch Status: 12800/60000             (21% | Loss: 0.651601\n",
      "Train Epoch: 4 | Batch Status: 16000/60000             (27% | Loss: 0.525115\n",
      "Train Epoch: 4 | Batch Status: 19200/60000             (32% | Loss: 0.647872\n",
      "Train Epoch: 4 | Batch Status: 22400/60000             (37% | Loss: 0.647996\n",
      "Train Epoch: 4 | Batch Status: 25600/60000             (43% | Loss: 0.865161\n",
      "Train Epoch: 4 | Batch Status: 28800/60000             (48% | Loss: 0.648929\n",
      "Train Epoch: 4 | Batch Status: 32000/60000             (53% | Loss: 0.505733\n",
      "Train Epoch: 4 | Batch Status: 35200/60000             (59% | Loss: 0.503849\n",
      "Train Epoch: 4 | Batch Status: 38400/60000             (64% | Loss: 0.720740\n",
      "Train Epoch: 4 | Batch Status: 41600/60000             (69% | Loss: 0.440938\n",
      "Train Epoch: 4 | Batch Status: 44800/60000             (75% | Loss: 0.884927\n",
      "Train Epoch: 4 | Batch Status: 48000/60000             (80% | Loss: 0.512128\n",
      "Train Epoch: 4 | Batch Status: 51200/60000             (85% | Loss: 0.865080\n",
      "Train Epoch: 4 | Batch Status: 54400/60000             (91% | Loss: 0.509608\n",
      "Train Epoch: 4 | Batch Status: 57600/60000             (96% | Loss: 0.339244\n",
      "Train Epoch: 5 | Batch Status: 0/60000             (0% | Loss: 0.814364\n",
      "Train Epoch: 5 | Batch Status: 3200/60000             (5% | Loss: 0.484468\n",
      "Train Epoch: 5 | Batch Status: 6400/60000             (11% | Loss: 0.795920\n",
      "Train Epoch: 5 | Batch Status: 9600/60000             (16% | Loss: 0.711460\n",
      "Train Epoch: 5 | Batch Status: 12800/60000             (21% | Loss: 0.729221\n",
      "Train Epoch: 5 | Batch Status: 16000/60000             (27% | Loss: 0.433483\n",
      "Train Epoch: 5 | Batch Status: 19200/60000             (32% | Loss: 1.112434\n",
      "Train Epoch: 5 | Batch Status: 22400/60000             (37% | Loss: 0.504451\n",
      "Train Epoch: 5 | Batch Status: 25600/60000             (43% | Loss: 0.720274\n",
      "Train Epoch: 5 | Batch Status: 28800/60000             (48% | Loss: 0.659710\n",
      "Train Epoch: 5 | Batch Status: 32000/60000             (53% | Loss: 1.086849\n",
      "Train Epoch: 5 | Batch Status: 35200/60000             (59% | Loss: 0.792040\n",
      "Train Epoch: 5 | Batch Status: 38400/60000             (64% | Loss: 0.733509\n",
      "Train Epoch: 5 | Batch Status: 41600/60000             (69% | Loss: 0.805832\n",
      "Train Epoch: 5 | Batch Status: 44800/60000             (75% | Loss: 0.604959\n",
      "Train Epoch: 5 | Batch Status: 48000/60000             (80% | Loss: 1.020508\n",
      "Train Epoch: 5 | Batch Status: 51200/60000             (85% | Loss: 0.434305\n",
      "Train Epoch: 5 | Batch Status: 54400/60000             (91% | Loss: 0.938151\n",
      "Train Epoch: 5 | Batch Status: 57600/60000             (96% | Loss: 0.506526\n",
      "=======================\n",
      " Test set: Average loss: 0.0229, Accuracy: 0.692\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch, cnn, ce_loss, train_loader, valid_loader, optimizer)\n",
    "test(cnn, ce_loss, test_loader)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
