{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a935b7fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\train-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80075430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "torch.Size([60000, 28, 28])\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset)) # 첫 번째 데이터의 개수를 파악한다.\n",
    "print(train_dataset.data.shape)# 두 번째 데이터의 shape를 파악한다.\n",
    "print(train_dataset.classes) # 세 번째 데이터의 클래스들을 파악한다.\n",
    "# 네 번째 모델의 입력과 출력 사이즈를 정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba6b50",
   "metadata": {},
   "source": [
    "## 퀴즈 (Easy)  \n",
    "Multi Layered Perceptrion 을 통해 MNIST 데이터셋을 분류하려면  \n",
    "1) 모델의 첫 번째 레이어의 shape 784  \n",
    "2) 모델의 출력크기는 어떻게 되어야할까요?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46bdfa5",
   "metadata": {},
   "source": [
    "## 퀴즈 (Normal)  \n",
    "간단한 MLP 모델을 구현해봅시다. \n",
    "1) 레이어 수는 총 4개로 (784, 512, 256, 128) 개의 뉴런이 존재합니다. \n",
    "  \n",
    "  \n",
    "2) 활성화함수는 relu를 사용합니다.  \n",
    "\n",
    "3) forward 함수에 x를 처음 입력받을 때 기존에 배운 flatten() 또는 reshape() 또는 view()를 활용해서 일차원 벡터로 변환하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca10996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화함수 사용하는 방법\n",
    "# x = F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a024664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 512)\n",
    "        self.l2 = nn.Linear(512, 256)\n",
    "        self.l3 = nn.Linear(256, 128)\n",
    "        self.l4 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the data (batch_size, 1, 28, 28)-> (batch_size, 784)\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        y_pred = F.softmax(self.l4(x))\n",
    "        # F.softmax()\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec2207",
   "metadata": {},
   "source": [
    "이제 train, test 함수를 작성해보겠습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b6ae02",
   "metadata": {},
   "source": [
    "## train, test 함수 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c193525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "567434\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "model = MLP().to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "def train(epoch, model, loss_func, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "            ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "\n",
    "def test(model, loss_func, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_count = 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        test_loss += loss_func(y_pred, y).item()\n",
    "        pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "        # torch.eq : Computes element-wise equality. return counts value\n",
    "        correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count/len(test_loader.dataset):.3}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f25547",
   "metadata": {},
   "source": [
    "## train 및 test 실행  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9834776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      " Test set: Average loss: 0.0721, Accuracy: 0.0958\n"
     ]
    }
   ],
   "source": [
    "test(model, ce_loss, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35e0616c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.301915\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 1.682014\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 1.712533\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (16% | Loss: 1.589786\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 1.642243\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (27% | Loss: 1.639403\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (32% | Loss: 1.572750\n",
      "Train Epoch: 1 | Batch Status: 22400/60000             (37% | Loss: 1.567739\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (43% | Loss: 1.563347\n",
      "Train Epoch: 1 | Batch Status: 28800/60000             (48% | Loss: 1.686077\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (53% | Loss: 1.612079\n",
      "Train Epoch: 1 | Batch Status: 35200/60000             (59% | Loss: 1.525046\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (64% | Loss: 1.514946\n",
      "Train Epoch: 1 | Batch Status: 41600/60000             (69% | Loss: 1.608708\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (75% | Loss: 1.492476\n",
      "Train Epoch: 1 | Batch Status: 48000/60000             (80% | Loss: 1.514843\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (85% | Loss: 1.585804\n",
      "Train Epoch: 1 | Batch Status: 54400/60000             (91% | Loss: 1.501300\n",
      "Train Epoch: 1 | Batch Status: 57600/60000             (96% | Loss: 1.544112\n",
      "=======================\n",
      " Test set: Average loss: 0.0477, Accuracy: 0.938\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,1):\n",
    "    train(epoch, model, ce_loss, train_loader, optimizer)\n",
    "    test(model, ce_loss, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67959d1",
   "metadata": {},
   "source": [
    "# MNIST Classification with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "376cb4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, C, W, H, K, S): # 채널, 너비, 높이, 커널 사이즈, 스트라이드\n",
    "        super(CNN, self).__init__()\n",
    "        # nn.Module에는 이미 conv 레이어가 구현되어 있다. \n",
    "        # 배치정규화도 구현되어있고 다 구현되어있습니다. \n",
    "        self.conv1 = nn.Conv2d(C, 32, kernel_size=K, stride=S)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=K, stride=S)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=K, stride=S)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=K, stride=S):\n",
    "            print((size - (kernel_size - 1) - 1) // stride + 1)\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(W, K, S)\n",
    "        convw = conv2d_size_out(convw, K, S)\n",
    "        convw = conv2d_size_out(convw, K, S)\n",
    "        \n",
    "        self.linear_input_size = convw * convw * 128\n",
    "        self.fc = nn.Linear(self.linear_input_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1) # (batch_size, flatten_size)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return F.log_softmax(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "679d7cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "6\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(C=1, W=28, H=28, K=3, S=2) \n",
    "cnn = cnn.to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75245488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98250\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in cnn.parameters())\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e3934f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.295867\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 0.512540\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 0.586473\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (16% | Loss: 0.749562\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 0.246079\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (27% | Loss: 0.457897\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (32% | Loss: 0.641234\n",
      "Train Epoch: 1 | Batch Status: 22400/60000             (37% | Loss: 0.601799\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (43% | Loss: 0.436592\n",
      "Train Epoch: 1 | Batch Status: 28800/60000             (48% | Loss: 0.588477\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (53% | Loss: 0.455620\n",
      "Train Epoch: 1 | Batch Status: 35200/60000             (59% | Loss: 0.710014\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (64% | Loss: 0.508702\n",
      "Train Epoch: 1 | Batch Status: 41600/60000             (69% | Loss: 0.524436\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (75% | Loss: 0.372404\n",
      "Train Epoch: 1 | Batch Status: 48000/60000             (80% | Loss: 0.539342\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (85% | Loss: 0.322597\n",
      "Train Epoch: 1 | Batch Status: 54400/60000             (91% | Loss: 0.224088\n",
      "Train Epoch: 1 | Batch Status: 57600/60000             (96% | Loss: 0.364152\n",
      "=======================\n",
      " Test set: Average loss: 0.0153, Accuracy: 0.796\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,1):\n",
    "    train(epoch, cnn, ce_loss, train_loader, optimizer)\n",
    "    test(cnn, ce_loss, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c095428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers, block, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # feature map size = 32x32x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 16x16x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 8x8x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        # self.avg_pool = nn.AvgPool2d(8, stride=1)\n",
    "        self.fc_out = nn.Linear(49 * 64, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "\n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "\n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "\n",
    "        #x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "\n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "\n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet_model():\n",
    "    block = ResidualBlock\n",
    "    model = ResNet(1, 3, block)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4c03895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300154\n"
     ]
    }
   ],
   "source": [
    "resnet = resnet_model()\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "total_params = sum(p.numel() for p in resnet.parameters())\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c02ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.971932\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 2.700802\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 3.128146\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (16% | Loss: 2.539647\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 2.866781\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (27% | Loss: 2.788840\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (32% | Loss: 3.067997\n",
      "Train Epoch: 1 | Batch Status: 22400/60000             (37% | Loss: 2.649715\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (43% | Loss: 2.787101\n",
      "Train Epoch: 1 | Batch Status: 28800/60000             (48% | Loss: 3.007999\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (53% | Loss: 2.727302\n",
      "Train Epoch: 1 | Batch Status: 35200/60000             (59% | Loss: 2.915809\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (64% | Loss: 2.796563\n",
      "Train Epoch: 1 | Batch Status: 41600/60000             (69% | Loss: 3.036546\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (75% | Loss: 2.472355\n",
      "Train Epoch: 1 | Batch Status: 48000/60000             (80% | Loss: 3.077345\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (85% | Loss: 3.175894\n",
      "Train Epoch: 1 | Batch Status: 54400/60000             (91% | Loss: 2.868106\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,1):\n",
    "    train(epoch, resnet, ce_loss, train_loader, optimizer)\n",
    "    test(resnet, ce_loss, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c31d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871838fa",
   "metadata": {},
   "source": [
    "## Assignment  \n",
    "\n",
    "1) cifar10 데이터셋, 데이터로더를 할당하는 코드를 실행해서 데이터셋을 다운받으세요.  \n",
    "2) 데이터셋의 채널과 모양을 확인해보세요. 그리고 채널과 모양은 CNN을 구현할 때 반영하세요.  \n",
    "3) conv layer가 5개인 CNN 모델을 구현하세요 모델의 총 파라미터 수는 약 30만개여야 합니다. 배치정규화를 사용하세요.    \n",
    "4) 구현한 CNN 모델을 학습시키세요. learning_rate=0.001, optimizer=Adam, Epochs=100  \n",
    "5) 구현되어 있는 Resnet 모델을 불러와서 학습시켜보세요. 옵티마이저와 하이퍼파라미터는 기존과 동일합니다.  \n",
    "6) 두 모델 중 어느 것이 더 좋은가요?  \n",
    "7) 필요하다면 구글링을 적극적으로 하세요!!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d6e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
