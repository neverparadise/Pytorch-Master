{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e9a575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing CNN.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile CNN.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, C, W, H, K, S): # 채널, 너비, 높이, 커널 사이즈, 스트라이드\n",
    "        super(CNN, self).__init__()\n",
    "        # nn.Module에는 이미 conv 레이어가 구현되어 있다. \n",
    "        # 배치정규화도 구현되어있고 다 구현되어있습니다. \n",
    "        self.conv1 = nn.Conv2d(C, 32, kernel_size=K, stride=S)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=K, stride=S)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=K, stride=S)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=K, stride=S):\n",
    "            print((size - (kernel_size - 1) - 1) // stride + 1)\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(W, K, S)\n",
    "        convw = conv2d_size_out(convw, K, S)\n",
    "        convw = conv2d_size_out(convw, K, S)\n",
    "        \n",
    "        self.linear_input_size = convw * convw * 128\n",
    "        self.fc = nn.Linear(self.linear_input_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1) # (batch_size, flatten_size)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99878f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ResNet.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers, block, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # feature map size = 32x32x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 16x16x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 8x8x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        # self.avg_pool = nn.AvgPool2d(8, stride=1)\n",
    "        self.fc_out = nn.Linear(49 * 64, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "\n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "\n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "\n",
    "        #x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "\n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "\n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet_model():\n",
    "    block = ResidualBlock\n",
    "    model = ResNet(1, 3, block)\n",
    "    return model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
