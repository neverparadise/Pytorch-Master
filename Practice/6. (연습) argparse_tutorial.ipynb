{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48bb1e5",
   "metadata": {},
   "source": [
    "## argparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e323951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 성적표 가중치 (한글, 수학, 영어) \n",
    "# 2. 고객 정보 저장 및 출력\n",
    "\n",
    "# argument parser : 파이썬 프로그램을 실행할 때 \n",
    "# 함수나 클래스에 사용할 원하는 argument를 가져오게 해줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b1bfbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--korean KOREAN]\n",
      "                             [--mathematcis MATHEMATCIS] [--english ENGLISH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\ye200\\AppData\\Roaming\\jupyter\\runtime\\kernel-3c09ac97-5502-4bbb-8da3-15d0de87fd80.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ye200\\anaconda3\\envs\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "# parser 정의\n",
    "parser = argparse.ArgumentParser(description='Argparse Tutorial')\n",
    "# add_argument()를 통해 argument의 이름, 타입, 기본 값, 도움말을 정의할 수 있다.\n",
    "parser.add_argument('-k', '--korean', type=int, default=0, help=\"Score of korean\")\n",
    "parser.add_argument('-m', '--mathematics', type=int, default=0, help=\"Score of mathematics\")\n",
    "parser.add_argument('-e', '--english', type=int, default=0, help=\"Score of english\")\n",
    "\n",
    "# add_argument() 함수를 호출하면 parser 인스턴ㅅ 내부에 해당 이름을 가지는 멤버변수를 생성\n",
    "# parse_arg()를 통해 프로그램 실행시 parser가 실행되도록 합니다.\n",
    "args = parser.parse_args()\n",
    "\n",
    "subject_info = {'korean': args.n}\n",
    "def average(args):\n",
    "    total_score = 0\n",
    "    total_score += args.korean\n",
    "    total_score += args.mathematics\n",
    "    total_score += args.english\n",
    "    print(total_score / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ca8470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: argparse_tutorial.py [-h] [-k KOREAN] [-m MATHEMATICS] [-e ENGLISH]\n",
      "\n",
      "Argparse Tutorial\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -k KOREAN, --korean KOREAN\n",
      "                        Score of korean\n",
      "  -m MATHEMATICS, --mathematics MATHEMATICS\n",
      "                        Score of mathematics\n",
      "  -e ENGLISH, --english ENGLISH\n",
      "                        Score of english\n"
     ]
    }
   ],
   "source": [
    "!python argparse_tutorial.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d01d5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n"
     ]
    }
   ],
   "source": [
    "!python argparse_tutorial.py --korean 70 --mathematics 80 --english 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e84fea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argparser는 코드 내부에서 자주 변경해야하는 값들을 미리 입력할 때 사용한다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62408d",
   "metadata": {},
   "source": [
    "## Quiz (Normal)  \n",
    "고객의 이름, 키, 발사이즈, 선호 브랜드 리스트를 입력 받아서 저장하고 출력하는 프로그램을 작성하세요.  \n",
    "argument의 종류는 --name, --height, --foot_size, --wish_list  \n",
    "각각 type은 string, float, int, int  \n",
    "각각 default는 \"홍길동\", 175.0, 270, [1,2,3,4]  \n",
    "파이썬 파일로 만들어서 여러분의 정보를 입력하고 출력하세요  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30c2ae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting argparse_tutorial3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile argparse_tutorial3.py \n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Argparse Tutorial2')\n",
    "parser.add_argument('--name', type=str, default=\"홍길동\", help=None)\n",
    "parser.add_argument('--height', type=float, default=175.0, help=None)\n",
    "parser.add_argument('--foot_size', type=int, default=270, help=None)\n",
    "parser.add_argument('--wish_list', type=list, default=[1, 2, 3, 4], help=None)\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(type(args))\n",
    "def print_user_info(args):\n",
    "    print(f\"{args.name}의 키는 {args.height}, 발사이즈는 {args.foot_size} 입니다.\")\n",
    "\n",
    "print_user_info(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32ff9036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ye200\\\\Aiffel\\\\Torch-Master\\\\Practice'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aa13611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'argparse.Namespace'>\n",
      "조해창의 키는 340.0, 발사이즈는 310 입니다.\n"
     ]
    }
   ],
   "source": [
    "!python argparse_tutorial3.py --name \"조해창\" --height 340 --foot_size 310"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f87b1b",
   "metadata": {},
   "source": [
    "## Quiz (Easy)  \n",
    "딥러닝 모델의 하이퍼파라미터에는 무엇이 있었나요?  \n",
    "epoch, learning_rate, batch_size, kernel_size, stride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0ff90",
   "metadata": {},
   "source": [
    "## Quiz (Easy)  \n",
    "argparser를 통해 아래의 파라미터를 입력 받을 수 있도록 파이썬 파일을 만들어보세요  \n",
    "파일의 이름은 run_cnn.py 로 만드세요.  \n",
    "위 블록의 내용을 활용해서 run_cnn의 상단에 하이퍼 파라미터들을 입력받는 argparser를 구현합니다.  \n",
    "입력받은 하이퍼파라미터들을 출력하세요. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8538236",
   "metadata": {},
   "source": [
    "## Quiz (Easy)\n",
    "아래 블록에 있는 모든 코드들을 복사해서 run_cnn 파이썬 파일에 붙여넣은 후,  \n",
    "입력 받은 하이퍼파라미터들을 적절한 위치에 할당하세요  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f75dceba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_cnn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_cnn.py \n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='run_cnn')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help=None)\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001, help=None)\n",
    "parser.add_argument('--epochs', type=int, default=5, help=None)\n",
    "parser.add_argument('--kernel_size', type=int, default=3, help=None)\n",
    "parser.add_argument('--stride', type=int, default=2, help=None)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = args.batch_size\n",
    "learning_rate = args.learning_rate\n",
    "epochs = args.epochs\n",
    "kernel_size = args.kernel_size\n",
    "stride = args.stride\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "valid_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "vaild_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "shape = train_dataset[0][0].shape\n",
    "print(shape)\n",
    "C = shape[0]\n",
    "W = shape[1]\n",
    "H = shape[2]\n",
    "print(C, W, H)\n",
    "\n",
    "def train(epoch, model, loss_func, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch+1} | Batch Status: {batch_index*len(x)}/{len(train_loader.dataset)} \\\n",
    "            ({100. * batch_index * batch_size / len(train_loader.dataset):.0f}% | Loss: {loss.item():.6f}')\n",
    "\n",
    "def test(model, loss_func, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_count = 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        test_loss += loss_func(y_pred, y).item()\n",
    "        pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "        # torch.eq : Computes element-wise equality. return counts value\n",
    "        correct_count += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'=======================\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct_count/len(test_loader.dataset):.3}')\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, C, W, H, K, S): # 채널, 너비, 높이, 커널 사이즈, 스트라이드\n",
    "        super(CNN, self).__init__()\n",
    "        # nn.Module에는 이미 conv 레이어가 구현되어 있다. \n",
    "        # 배치정규화도 구현되어있고 다 구현되어있습니다. \n",
    "        self.conv1 = nn.Conv2d(C, 32, kernel_size=K, stride=S)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=K, stride=S)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=K, stride=S)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=K, stride=S):\n",
    "            print((size - (kernel_size - 1) - 1) // stride + 1)\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(W, K, S)\n",
    "        convw = conv2d_size_out(convw, K, S)\n",
    "        convw = conv2d_size_out(convw, K, S)\n",
    "        \n",
    "        self.linear_input_size = convw * convw * 128\n",
    "        self.fc = nn.Linear(self.linear_input_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1) # (batch_size, flatten_size)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "cnn = CNN(C=C, W=W, H=H, K=kernel_size, S=stride) \n",
    "cnn = cnn.to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)    \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(epoch, cnn, ce_loss, train_loader, optimizer)\n",
    "\n",
    "test(cnn, ce_loss, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e166c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_cnn.py:102: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "1 28 28\n",
      "13\n",
      "6\n",
      "2\n",
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.285835\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 0.151139\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 0.081851\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (32% | Loss: 0.119689\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (43% | Loss: 0.034239\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (53% | Loss: 0.181921\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (64% | Loss: 0.073772\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (75% | Loss: 0.034838\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (85% | Loss: 0.053112\n",
      "Train Epoch: 1 | Batch Status: 57600/60000             (96% | Loss: 0.031378\n",
      "Train Epoch: 2 | Batch Status: 0/60000             (0% | Loss: 0.122599\n",
      "Train Epoch: 2 | Batch Status: 6400/60000             (11% | Loss: 0.011364\n",
      "Train Epoch: 2 | Batch Status: 12800/60000             (21% | Loss: 0.002075\n",
      "Train Epoch: 2 | Batch Status: 19200/60000             (32% | Loss: 0.021105\n",
      "Train Epoch: 2 | Batch Status: 25600/60000             (43% | Loss: 0.007781\n",
      "Train Epoch: 2 | Batch Status: 32000/60000             (53% | Loss: 0.046152\n",
      "Train Epoch: 2 | Batch Status: 38400/60000             (64% | Loss: 0.005202\n",
      "Train Epoch: 2 | Batch Status: 44800/60000             (75% | Loss: 0.024482\n",
      "Train Epoch: 2 | Batch Status: 51200/60000             (85% | Loss: 0.010059\n",
      "Train Epoch: 2 | Batch Status: 57600/60000             (96% | Loss: 0.014894\n",
      "Train Epoch: 3 | Batch Status: 0/60000             (0% | Loss: 0.003505\n",
      "Train Epoch: 3 | Batch Status: 6400/60000             (11% | Loss: 0.002069\n",
      "Train Epoch: 3 | Batch Status: 12800/60000             (21% | Loss: 0.071762\n",
      "Train Epoch: 3 | Batch Status: 19200/60000             (32% | Loss: 0.004445\n",
      "Train Epoch: 3 | Batch Status: 25600/60000             (43% | Loss: 0.018449\n",
      "Train Epoch: 3 | Batch Status: 32000/60000             (53% | Loss: 0.055432\n",
      "Train Epoch: 3 | Batch Status: 38400/60000             (64% | Loss: 0.034665\n",
      "Train Epoch: 3 | Batch Status: 44800/60000             (75% | Loss: 0.021848\n",
      "Train Epoch: 3 | Batch Status: 51200/60000             (85% | Loss: 0.059136\n",
      "Train Epoch: 3 | Batch Status: 57600/60000             (96% | Loss: 0.048342\n",
      "=======================\n",
      " Test set: Average loss: 0.0006, Accuracy: 0.988\n",
      "torch.Size([1, 28, 28])\n",
      "1 28 28\n",
      "13\n",
      "6\n",
      "2\n",
      "Train Epoch: 1 | Batch Status: 0/60000             (0% | Loss: 2.350377\n",
      "Train Epoch: 1 | Batch Status: 1600/60000             (3% | Loss: 1.184593\n",
      "Train Epoch: 1 | Batch Status: 3200/60000             (5% | Loss: 1.047650\n",
      "Train Epoch: 1 | Batch Status: 4800/60000             (8% | Loss: 1.479222\n",
      "Train Epoch: 1 | Batch Status: 6400/60000             (11% | Loss: 0.925487\n",
      "Train Epoch: 1 | Batch Status: 8000/60000             (13% | Loss: 1.185899\n",
      "Train Epoch: 1 | Batch Status: 9600/60000             (16% | Loss: 1.571553\n",
      "Train Epoch: 1 | Batch Status: 11200/60000             (19% | Loss: 1.249435\n",
      "Train Epoch: 1 | Batch Status: 12800/60000             (21% | Loss: 1.557732\n",
      "Train Epoch: 1 | Batch Status: 14400/60000             (24% | Loss: 1.139668\n",
      "Train Epoch: 1 | Batch Status: 16000/60000             (27% | Loss: 0.829292\n",
      "Train Epoch: 1 | Batch Status: 17600/60000             (29% | Loss: 1.151187\n",
      "Train Epoch: 1 | Batch Status: 19200/60000             (32% | Loss: 0.871613\n",
      "Train Epoch: 1 | Batch Status: 20800/60000             (35% | Loss: 1.303749\n",
      "Train Epoch: 1 | Batch Status: 22400/60000             (37% | Loss: 1.447037\n",
      "Train Epoch: 1 | Batch Status: 24000/60000             (40% | Loss: 1.460764\n",
      "Train Epoch: 1 | Batch Status: 25600/60000             (43% | Loss: 0.809756\n",
      "Train Epoch: 1 | Batch Status: 27200/60000             (45% | Loss: 0.870032\n",
      "Train Epoch: 1 | Batch Status: 28800/60000             (48% | Loss: 1.319702\n",
      "Train Epoch: 1 | Batch Status: 30400/60000             (51% | Loss: 1.152041\n",
      "Train Epoch: 1 | Batch Status: 32000/60000             (53% | Loss: 1.110718\n",
      "Train Epoch: 1 | Batch Status: 33600/60000             (56% | Loss: 1.066365\n",
      "Train Epoch: 1 | Batch Status: 35200/60000             (59% | Loss: 1.252863\n",
      "Train Epoch: 1 | Batch Status: 36800/60000             (61% | Loss: 1.299459\n",
      "Train Epoch: 1 | Batch Status: 38400/60000             (64% | Loss: 1.336010\n",
      "Train Epoch: 1 | Batch Status: 40000/60000             (67% | Loss: 1.170849\n",
      "Train Epoch: 1 | Batch Status: 41600/60000             (69% | Loss: 1.295984\n",
      "Train Epoch: 1 | Batch Status: 43200/60000             (72% | Loss: 1.310082\n",
      "Train Epoch: 1 | Batch Status: 44800/60000             (75% | Loss: 1.015699\n",
      "Train Epoch: 1 | Batch Status: 46400/60000             (77% | Loss: 1.186127\n",
      "Train Epoch: 1 | Batch Status: 48000/60000             (80% | Loss: 1.442923\n",
      "Train Epoch: 1 | Batch Status: 49600/60000             (83% | Loss: 1.152830\n",
      "Train Epoch: 1 | Batch Status: 51200/60000             (85% | Loss: 1.242987\n",
      "Train Epoch: 1 | Batch Status: 52800/60000             (88% | Loss: 0.917742\n",
      "Train Epoch: 1 | Batch Status: 54400/60000             (91% | Loss: 1.411154\n",
      "Train Epoch: 1 | Batch Status: 56000/60000             (93% | Loss: 1.047331\n",
      "Train Epoch: 1 | Batch Status: 57600/60000             (96% | Loss: 1.283283\n",
      "Train Epoch: 1 | Batch Status: 59200/60000             (99% | Loss: 1.453822\n",
      "Train Epoch: 2 | Batch Status: 0/60000             (0% | Loss: 1.763171\n",
      "Train Epoch: 2 | Batch Status: 1600/60000             (3% | Loss: 1.011137\n",
      "Train Epoch: 2 | Batch Status: 3200/60000             (5% | Loss: 1.444542\n",
      "Train Epoch: 2 | Batch Status: 4800/60000             (8% | Loss: 1.016110\n",
      "Train Epoch: 2 | Batch Status: 6400/60000             (11% | Loss: 1.022411"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_cnn.py:102: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 2 | Batch Status: 8000/60000             (13% | Loss: 0.580096\n",
      "Train Epoch: 2 | Batch Status: 9600/60000             (16% | Loss: 1.365226\n",
      "Train Epoch: 2 | Batch Status: 11200/60000             (19% | Loss: 1.013435\n",
      "Train Epoch: 2 | Batch Status: 12800/60000             (21% | Loss: 1.152822\n",
      "Train Epoch: 2 | Batch Status: 14400/60000             (24% | Loss: 0.590774\n",
      "Train Epoch: 2 | Batch Status: 16000/60000             (27% | Loss: 0.932521\n",
      "Train Epoch: 2 | Batch Status: 17600/60000             (29% | Loss: 1.583625\n",
      "Train Epoch: 2 | Batch Status: 19200/60000             (32% | Loss: 0.750975\n",
      "Train Epoch: 2 | Batch Status: 20800/60000             (35% | Loss: 1.301661\n",
      "Train Epoch: 2 | Batch Status: 22400/60000             (37% | Loss: 1.583241\n",
      "Train Epoch: 2 | Batch Status: 24000/60000             (40% | Loss: 1.053894\n",
      "Train Epoch: 2 | Batch Status: 25600/60000             (43% | Loss: 1.583348\n",
      "Train Epoch: 2 | Batch Status: 27200/60000             (45% | Loss: 1.204703\n",
      "Train Epoch: 2 | Batch Status: 28800/60000             (48% | Loss: 0.450433\n",
      "Train Epoch: 2 | Batch Status: 30400/60000             (51% | Loss: 0.731298\n",
      "Train Epoch: 2 | Batch Status: 32000/60000             (53% | Loss: 1.335524\n",
      "Train Epoch: 2 | Batch Status: 33600/60000             (56% | Loss: 1.329284\n",
      "Train Epoch: 2 | Batch Status: 35200/60000             (59% | Loss: 1.157188\n",
      "Train Epoch: 2 | Batch Status: 36800/60000             (61% | Loss: 1.164782\n",
      "Train Epoch: 2 | Batch Status: 38400/60000             (64% | Loss: 1.655281\n",
      "Train Epoch: 2 | Batch Status: 40000/60000             (67% | Loss: 1.297111\n",
      "Train Epoch: 2 | Batch Status: 41600/60000             (69% | Loss: 1.439510\n",
      "Train Epoch: 2 | Batch Status: 43200/60000             (72% | Loss: 1.160464\n",
      "Train Epoch: 2 | Batch Status: 44800/60000             (75% | Loss: 0.720067\n",
      "Train Epoch: 2 | Batch Status: 46400/60000             (77% | Loss: 1.026971\n",
      "Train Epoch: 2 | Batch Status: 48000/60000             (80% | Loss: 1.295924\n",
      "Train Epoch: 2 | Batch Status: 49600/60000             (83% | Loss: 1.791565\n",
      "Train Epoch: 2 | Batch Status: 51200/60000             (85% | Loss: 1.439392\n",
      "Train Epoch: 2 | Batch Status: 52800/60000             (88% | Loss: 1.446733\n",
      "Train Epoch: 2 | Batch Status: 54400/60000             (91% | Loss: 1.172254\n",
      "Train Epoch: 2 | Batch Status: 56000/60000             (93% | Loss: 1.152433\n",
      "Train Epoch: 2 | Batch Status: 57600/60000             (96% | Loss: 0.816015\n",
      "Train Epoch: 2 | Batch Status: 59200/60000             (99% | Loss: 1.378741\n",
      "Train Epoch: 3 | Batch Status: 0/60000             (0% | Loss: 1.295889\n",
      "Train Epoch: 3 | Batch Status: 1600/60000             (3% | Loss: 0.586290\n",
      "Train Epoch: 3 | Batch Status: 3200/60000             (5% | Loss: 1.336671\n",
      "Train Epoch: 3 | Batch Status: 4800/60000             (8% | Loss: 1.030502\n",
      "Train Epoch: 3 | Batch Status: 6400/60000             (11% | Loss: 1.151891\n",
      "Train Epoch: 3 | Batch Status: 8000/60000             (13% | Loss: 1.295574\n",
      "Train Epoch: 3 | Batch Status: 9600/60000             (16% | Loss: 1.007734\n",
      "Train Epoch: 3 | Batch Status: 11200/60000             (19% | Loss: 0.580276\n",
      "Train Epoch: 3 | Batch Status: 12800/60000             (21% | Loss: 1.321051\n",
      "Train Epoch: 3 | Batch Status: 14400/60000             (24% | Loss: 1.295411\n",
      "Train Epoch: 3 | Batch Status: 16000/60000             (27% | Loss: 1.439350\n",
      "Train Epoch: 3 | Batch Status: 17600/60000             (29% | Loss: 1.151764\n",
      "Train Epoch: 3 | Batch Status: 19200/60000             (32% | Loss: 1.155309\n",
      "Train Epoch: 3 | Batch Status: 20800/60000             (35% | Loss: 1.240137\n",
      "Train Epoch: 3 | Batch Status: 22400/60000             (37% | Loss: 1.303783\n",
      "Train Epoch: 3 | Batch Status: 24000/60000             (40% | Loss: 1.296179\n",
      "Train Epoch: 3 | Batch Status: 25600/60000             (43% | Loss: 1.440307\n",
      "Train Epoch: 3 | Batch Status: 27200/60000             (45% | Loss: 0.870079\n",
      "Train Epoch: 3 | Batch Status: 28800/60000             (48% | Loss: 0.721527\n",
      "Train Epoch: 3 | Batch Status: 30400/60000             (51% | Loss: 0.312427\n",
      "Train Epoch: 3 | Batch Status: 32000/60000             (53% | Loss: 1.441901\n",
      "Train Epoch: 3 | Batch Status: 33600/60000             (56% | Loss: 0.864652\n",
      "Train Epoch: 3 | Batch Status: 35200/60000             (59% | Loss: 1.295737\n",
      "Train Epoch: 3 | Batch Status: 36800/60000             (61% | Loss: 1.648344\n",
      "Train Epoch: 3 | Batch Status: 38400/60000             (64% | Loss: 0.864664\n",
      "Train Epoch: 3 | Batch Status: 40000/60000             (67% | Loss: 0.863744\n",
      "Train Epoch: 3 | Batch Status: 41600/60000             (69% | Loss: 1.043917\n",
      "Train Epoch: 3 | Batch Status: 43200/60000             (72% | Loss: 0.864301\n",
      "Train Epoch: 3 | Batch Status: 44800/60000             (75% | Loss: 1.151455\n",
      "Train Epoch: 3 | Batch Status: 46400/60000             (77% | Loss: 1.023536\n",
      "Train Epoch: 3 | Batch Status: 48000/60000             (80% | Loss: 1.013423\n",
      "Train Epoch: 3 | Batch Status: 49600/60000             (83% | Loss: 1.345750\n",
      "Train Epoch: 3 | Batch Status: 51200/60000             (85% | Loss: 0.863950\n",
      "Train Epoch: 3 | Batch Status: 52800/60000             (88% | Loss: 1.007911\n",
      "Train Epoch: 3 | Batch Status: 54400/60000             (91% | Loss: 0.863671\n",
      "Train Epoch: 3 | Batch Status: 56000/60000             (93% | Loss: 0.869807\n",
      "Train Epoch: 3 | Batch Status: 57600/60000             (96% | Loss: 1.200181\n",
      "Train Epoch: 3 | Batch Status: 59200/60000             (99% | Loss: 0.865476\n",
      "=======================\n",
      " Test set: Average loss: 0.0709, Accuracy: 0.61\n"
     ]
    }
   ],
   "source": [
    "!python run_cnn.py --batch_size 16 --epochs 3 --learning_rate 0.001 --stride 2 --kernel_size 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(input())\n",
    "epochs = int(input())\n",
    "learning_rate = float(input())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
